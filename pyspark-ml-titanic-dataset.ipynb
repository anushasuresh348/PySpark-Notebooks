{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b1d23c4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-02T09:23:28.638248Z",
     "iopub.status.busy": "2022-08-02T09:23:28.637478Z",
     "iopub.status.idle": "2022-08-02T09:23:28.652752Z",
     "shell.execute_reply": "2022-08-02T09:23:28.651321Z"
    },
    "papermill": {
     "duration": 0.036706,
     "end_time": "2022-08-02T09:23:28.655623",
     "exception": false,
     "start_time": "2022-08-02T09:23:28.618917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/train.csv\n",
      "/kaggle/input/titanic/test.csv\n",
      "/kaggle/input/titanic/gender_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a078a849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:23:28.690030Z",
     "iopub.status.busy": "2022-08-02T09:23:28.689334Z",
     "iopub.status.idle": "2022-08-02T09:24:03.526889Z",
     "shell.execute_reply": "2022-08-02T09:24:03.525390Z"
    },
    "papermill": {
     "duration": 34.856812,
     "end_time": "2022-08-02T09:24:03.529798",
     "exception": false,
     "start_time": "2022-08-02T09:23:28.672986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: spark-3.0.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\r\n",
      "tar: Error is not recoverable: exiting now\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Pyspark & Setup our Java Environment - Takes less than a minute\n",
    "\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
    "!tar -xvf spark-3.0.1-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31833d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:24:03.564669Z",
     "iopub.status.busy": "2022-08-02T09:24:03.564242Z",
     "iopub.status.idle": "2022-08-02T09:25:11.410303Z",
     "shell.execute_reply": "2022-08-02T09:25:11.408539Z"
    },
    "papermill": {
     "duration": 67.867499,
     "end_time": "2022-08-02T09:25:11.413360",
     "exception": false,
     "start_time": "2022-08-02T09:24:03.545861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=4524273651170e81ede69bbcde0bac8dcc1750033b744f1c54f0d438897afd42\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.3.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "## installing pyspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03efe229",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:11.506474Z",
     "iopub.status.busy": "2022-08-02T09:25:11.505615Z",
     "iopub.status.idle": "2022-08-02T09:25:22.895139Z",
     "shell.execute_reply": "2022-08-02T09:25:22.893462Z"
    },
    "papermill": {
     "duration": 11.440107,
     "end_time": "2022-08-02T09:25:22.898257",
     "exception": false,
     "start_time": "2022-08-02T09:25:11.458150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (8.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.7/site-packages (from pyarrow) (1.21.6)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "## installing pyarrow\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59af89b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:22.991892Z",
     "iopub.status.busy": "2022-08-02T09:25:22.991420Z",
     "iopub.status.idle": "2022-08-02T09:25:29.060301Z",
     "shell.execute_reply": "2022-08-02T09:25:29.059114Z"
    },
    "papermill": {
     "duration": 6.119051,
     "end_time": "2022-08-02T09:25:29.063629",
     "exception": false,
     "start_time": "2022-08-02T09:25:22.944578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:25:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "## creating a spark session\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Spark ML on Titanic dataset\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01146656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:29.156200Z",
     "iopub.status.busy": "2022-08-02T09:25:29.155711Z",
     "iopub.status.idle": "2022-08-02T09:25:36.794149Z",
     "shell.execute_reply": "2022-08-02T09:25:36.792934Z"
    },
    "papermill": {
     "duration": 7.688246,
     "end_time": "2022-08-02T09:25:36.797487",
     "exception": false,
     "start_time": "2022-08-02T09:25:29.109241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Reading the dataset\n",
    "df_train = spark.read.csv('/kaggle/input/titanic/train.csv', header = True, inferSchema=True)\n",
    "df_test = spark.read.csv('/kaggle/input/titanic/test.csv', header = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e87485d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:36.926923Z",
     "iopub.status.busy": "2022-08-02T09:25:36.926365Z",
     "iopub.status.idle": "2022-08-02T09:25:37.409651Z",
     "shell.execute_reply": "2022-08-02T09:25:37.408540Z"
    },
    "papermill": {
     "duration": 0.551958,
     "end_time": "2022-08-02T09:25:37.413735",
     "exception": false,
     "start_time": "2022-08-02T09:25:36.861777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d8ec79c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:37.553352Z",
     "iopub.status.busy": "2022-08-02T09:25:37.552748Z",
     "iopub.status.idle": "2022-08-02T09:25:37.567350Z",
     "shell.execute_reply": "2022-08-02T09:25:37.565907Z"
    },
    "papermill": {
     "duration": 0.086336,
     "end_time": "2022-08-02T09:25:37.570747",
     "exception": false,
     "start_time": "2022-08-02T09:25:37.484411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99e941ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:37.665856Z",
     "iopub.status.busy": "2022-08-02T09:25:37.665054Z",
     "iopub.status.idle": "2022-08-02T09:25:37.710049Z",
     "shell.execute_reply": "2022-08-02T09:25:37.708658Z"
    },
    "papermill": {
     "duration": 0.096452,
     "end_time": "2022-08-02T09:25:37.713736",
     "exception": false,
     "start_time": "2022-08-02T09:25:37.617284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('PassengerId', IntegerType(), True), StructField('Survived', IntegerType(), True), StructField('Pclass', IntegerType(), True), StructField('Name', StringType(), True), StructField('Sex', StringType(), True), StructField('Age', DoubleType(), True), StructField('SibSp', IntegerType(), True), StructField('Parch', IntegerType(), True), StructField('Ticket', StringType(), True), StructField('Fare', DoubleType(), True), StructField('Cabin', StringType(), True), StructField('Embarked', StringType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4940b54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:37.815995Z",
     "iopub.status.busy": "2022-08-02T09:25:37.814999Z",
     "iopub.status.idle": "2022-08-02T09:25:37.827623Z",
     "shell.execute_reply": "2022-08-02T09:25:37.825973Z"
    },
    "papermill": {
     "duration": 0.064312,
     "end_time": "2022-08-02T09:25:37.830777",
     "exception": false,
     "start_time": "2022-08-02T09:25:37.766465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2942c97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:37.926984Z",
     "iopub.status.busy": "2022-08-02T09:25:37.926200Z",
     "iopub.status.idle": "2022-08-02T09:25:37.934340Z",
     "shell.execute_reply": "2022-08-02T09:25:37.933002Z"
    },
    "papermill": {
     "duration": 0.059246,
     "end_time": "2022-08-02T09:25:37.936841",
     "exception": false,
     "start_time": "2022-08-02T09:25:37.877595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PassengerId', 'int'),\n",
       " ('Survived', 'int'),\n",
       " ('Pclass', 'int'),\n",
       " ('Name', 'string'),\n",
       " ('Sex', 'string'),\n",
       " ('Age', 'double'),\n",
       " ('SibSp', 'int'),\n",
       " ('Parch', 'int'),\n",
       " ('Ticket', 'string'),\n",
       " ('Fare', 'double'),\n",
       " ('Cabin', 'string'),\n",
       " ('Embarked', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e68f2152",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:38.032167Z",
     "iopub.status.busy": "2022-08-02T09:25:38.031419Z",
     "iopub.status.idle": "2022-08-02T09:25:40.862315Z",
     "shell.execute_reply": "2022-08-02T09:25:40.859772Z"
    },
    "papermill": {
     "duration": 2.881873,
     "end_time": "2022-08-02T09:25:40.865675",
     "exception": false,
     "start_time": "2022-08-02T09:25:37.983802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|\n",
      "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|\n",
      "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6201b86c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:41.008319Z",
     "iopub.status.busy": "2022-08-02T09:25:41.007616Z",
     "iopub.status.idle": "2022-08-02T09:25:41.233432Z",
     "shell.execute_reply": "2022-08-02T09:25:41.232260Z"
    },
    "papermill": {
     "duration": 0.300982,
     "end_time": "2022-08-02T09:25:41.238683",
     "exception": false,
     "start_time": "2022-08-02T09:25:40.937701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+\n",
      "|Survived|Pclass|Embarked|\n",
      "+--------+------+--------+\n",
      "|       0|     3|       S|\n",
      "|       1|     1|       C|\n",
      "|       1|     3|       S|\n",
      "|       1|     1|       S|\n",
      "|       0|     3|       S|\n",
      "|       0|     3|       Q|\n",
      "|       0|     1|       S|\n",
      "|       0|     3|       S|\n",
      "|       1|     3|       S|\n",
      "|       1|     2|       C|\n",
      "|       1|     3|       S|\n",
      "|       1|     1|       S|\n",
      "|       0|     3|       S|\n",
      "|       0|     3|       S|\n",
      "|       0|     3|       S|\n",
      "|       1|     2|       S|\n",
      "|       0|     3|       Q|\n",
      "|       1|     2|       S|\n",
      "|       0|     3|       S|\n",
      "|       1|     3|       C|\n",
      "+--------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.select(\"Survived\",\"Pclass\",\"Embarked\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d768f",
   "metadata": {
    "papermill": {
     "duration": 0.067711,
     "end_time": "2022-08-02T09:25:41.377331",
     "exception": false,
     "start_time": "2022-08-02T09:25:41.309620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab038b27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:41.519435Z",
     "iopub.status.busy": "2022-08-02T09:25:41.519050Z",
     "iopub.status.idle": "2022-08-02T09:25:42.065194Z",
     "shell.execute_reply": "2022-08-02T09:25:42.063910Z"
    },
    "papermill": {
     "duration": 0.625902,
     "end_time": "2022-08-02T09:25:42.072018",
     "exception": false,
     "start_time": "2022-08-02T09:25:41.446116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupBy(\"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d563fc",
   "metadata": {
    "papermill": {
     "duration": 0.065566,
     "end_time": "2022-08-02T09:25:42.187383",
     "exception": false,
     "start_time": "2022-08-02T09:25:42.121817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "342 passengers survived and 549 did not survive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d7637b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:42.339645Z",
     "iopub.status.busy": "2022-08-02T09:25:42.339244Z",
     "iopub.status.idle": "2022-08-02T09:25:42.683012Z",
     "shell.execute_reply": "2022-08-02T09:25:42.681736Z"
    },
    "papermill": {
     "duration": 0.424489,
     "end_time": "2022-08-02T09:25:42.686945",
     "exception": false,
     "start_time": "2022-08-02T09:25:42.262456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|   Sex|Survived|count|\n",
      "+------+--------+-----+\n",
      "|  male|       0|  468|\n",
      "|female|       1|  233|\n",
      "|female|       0|   81|\n",
      "|  male|       1|  109|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupBy(\"Sex\",\"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a05faf",
   "metadata": {
    "papermill": {
     "duration": 0.071022,
     "end_time": "2022-08-02T09:25:42.837541",
     "exception": false,
     "start_time": "2022-08-02T09:25:42.766519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "out of 342 survived passengers, 233 are female and rest male; out of 549 deaths, 81 are female and rest male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f88cdf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:42.977784Z",
     "iopub.status.busy": "2022-08-02T09:25:42.976264Z",
     "iopub.status.idle": "2022-08-02T09:25:43.512143Z",
     "shell.execute_reply": "2022-08-02T09:25:43.510594Z"
    },
    "papermill": {
     "duration": 0.609034,
     "end_time": "2022-08-02T09:25:43.515769",
     "exception": false,
     "start_time": "2022-08-02T09:25:42.906735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|Pclass|Survived|count|\n",
      "+------+--------+-----+\n",
      "|     1|       0|   80|\n",
      "|     3|       1|  119|\n",
      "|     1|       1|  136|\n",
      "|     2|       1|   87|\n",
      "|     2|       0|   97|\n",
      "|     3|       0|  372|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupBy(\"Pclass\",\"Survived\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54639304",
   "metadata": {
    "papermill": {
     "duration": 0.068013,
     "end_time": "2022-08-02T09:25:43.673318",
     "exception": false,
     "start_time": "2022-08-02T09:25:43.605305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "out of 342 survived, 136 belong to 1st class, 87 to 2nd class and 119 to 3rd class\n",
    "\n",
    "out of 549 deaths, 80 belong to 1st class,97 to second class and 372 to 3rd class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9acca",
   "metadata": {
    "papermill": {
     "duration": 0.075138,
     "end_time": "2022-08-02T09:25:43.818039",
     "exception": false,
     "start_time": "2022-08-02T09:25:43.742901",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**PREPROCESSING THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3775dcbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:43.952063Z",
     "iopub.status.busy": "2022-08-02T09:25:43.951545Z",
     "iopub.status.idle": "2022-08-02T09:25:43.960215Z",
     "shell.execute_reply": "2022-08-02T09:25:43.959114Z"
    },
    "papermill": {
     "duration": 0.076587,
     "end_time": "2022-08-02T09:25:43.962484",
     "exception": false,
     "start_time": "2022-08-02T09:25:43.885897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function use to print feature with null values and null count \n",
    "\n",
    "from pyspark.sql.functions import mean,col,split,regexp_extract,when, lit\n",
    "\n",
    "def null_value_count(df):\n",
    "  null_columns_counts = []\n",
    "  numRows = df.count()\n",
    "  for k in df.columns:\n",
    "    nullRows = df.where(col(k).isNull()).count()\n",
    "    if(nullRows > 0):\n",
    "      temp = k,nullRows\n",
    "      null_columns_counts.append(temp)\n",
    "  return(null_columns_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bd98fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:44.060206Z",
     "iopub.status.busy": "2022-08-02T09:25:44.059278Z",
     "iopub.status.idle": "2022-08-02T09:25:46.369394Z",
     "shell.execute_reply": "2022-08-02T09:25:46.368267Z"
    },
    "papermill": {
     "duration": 2.362862,
     "end_time": "2022-08-02T09:25:46.372934",
     "exception": false,
     "start_time": "2022-08-02T09:25:44.010072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Age', 177), ('Cabin', 687), ('Embarked', 2)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling function\n",
    "null_columns_count_list = null_value_count(df_train)\n",
    "null_columns_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c00d2079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:46.526369Z",
     "iopub.status.busy": "2022-08-02T09:25:46.525987Z",
     "iopub.status.idle": "2022-08-02T09:25:47.604380Z",
     "shell.execute_reply": "2022-08-02T09:25:47.602591Z"
    },
    "papermill": {
     "duration": 1.152556,
     "end_time": "2022-08-02T09:25:47.607557",
     "exception": false,
     "start_time": "2022-08-02T09:25:46.455001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------+\n",
      "|Column_With_Null_Value|Null_Values_Count|\n",
      "+----------------------+-----------------+\n",
      "|                   Age|              177|\n",
      "|                 Cabin|              687|\n",
      "|              Embarked|                2|\n",
      "+----------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(null_columns_count_list, ['Column_With_Null_Value', 'Null_Values_Count']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bca51596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:47.707623Z",
     "iopub.status.busy": "2022-08-02T09:25:47.707254Z",
     "iopub.status.idle": "2022-08-02T09:25:47.922396Z",
     "shell.execute_reply": "2022-08-02T09:25:47.920785Z"
    },
    "papermill": {
     "duration": 0.266512,
     "end_time": "2022-08-02T09:25:47.925546",
     "exception": false,
     "start_time": "2022-08-02T09:25:47.659034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.69911764705882\n"
     ]
    }
   ],
   "source": [
    "mean_age = df_train.select(mean('Age')).collect()[0][0]\n",
    "print(mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbf28dc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:48.022285Z",
     "iopub.status.busy": "2022-08-02T09:25:48.021887Z",
     "iopub.status.idle": "2022-08-02T09:25:48.302567Z",
     "shell.execute_reply": "2022-08-02T09:25:48.301344Z"
    },
    "papermill": {
     "duration": 0.333628,
     "end_time": "2022-08-02T09:25:48.307223",
     "exception": false,
     "start_time": "2022-08-02T09:25:47.973595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       Q|   77|\n",
      "|    null|    2|\n",
      "|       C|  168|\n",
      "|       S|  644|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupBy(\"Embarked\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f378fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:48.429280Z",
     "iopub.status.busy": "2022-08-02T09:25:48.428855Z",
     "iopub.status.idle": "2022-08-02T09:25:48.454037Z",
     "shell.execute_reply": "2022-08-02T09:25:48.452553Z"
    },
    "papermill": {
     "duration": 0.079675,
     "end_time": "2022-08-02T09:25:48.457710",
     "exception": false,
     "start_time": "2022-08-02T09:25:48.378035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_train.na.fill({\"Embarked\" : 'S'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acfb239a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:48.559576Z",
     "iopub.status.busy": "2022-08-02T09:25:48.558417Z",
     "iopub.status.idle": "2022-08-02T09:25:48.572965Z",
     "shell.execute_reply": "2022-08-02T09:25:48.571626Z"
    },
    "papermill": {
     "duration": 0.068834,
     "end_time": "2022-08-02T09:25:48.578491",
     "exception": false,
     "start_time": "2022-08-02T09:25:48.509657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's drop Cabin features as it has lots of null values\n",
    "df_train = df_train.drop(\"Cabin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1987af56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:48.725721Z",
     "iopub.status.busy": "2022-08-02T09:25:48.724663Z",
     "iopub.status.idle": "2022-08-02T09:25:48.897059Z",
     "shell.execute_reply": "2022-08-02T09:25:48.895884Z"
    },
    "papermill": {
     "duration": 0.248329,
     "end_time": "2022-08-02T09:25:48.900659",
     "exception": false,
     "start_time": "2022-08-02T09:25:48.652330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fdaeafe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:49.022211Z",
     "iopub.status.busy": "2022-08-02T09:25:49.021669Z",
     "iopub.status.idle": "2022-08-02T09:25:49.048516Z",
     "shell.execute_reply": "2022-08-02T09:25:49.047555Z"
    },
    "papermill": {
     "duration": 0.078702,
     "end_time": "2022-08-02T09:25:49.051169",
     "exception": false,
     "start_time": "2022-08-02T09:25:48.972467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the Regex \"\"[A-Za-z]+).\" we extract the initials from the Name. \n",
    "# It looks for strings which lie between A-Z or a-z and followed by a .(dot).\n",
    "df_train = df_train.withColumn(\"Initial\",regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "176046a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:49.149969Z",
     "iopub.status.busy": "2022-08-02T09:25:49.149116Z",
     "iopub.status.idle": "2022-08-02T09:25:49.343482Z",
     "shell.execute_reply": "2022-08-02T09:25:49.342294Z"
    },
    "papermill": {
     "duration": 0.250274,
     "end_time": "2022-08-02T09:25:49.348994",
     "exception": false,
     "start_time": "2022-08-02T09:25:49.098720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|Initial|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|       S|     Mr|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|       C|    Mrs|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|       S|   Miss|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|       S|    Mrs|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|       S|     Mr|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583|       Q|     Mr|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|       S|     Mr|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075|       S| Master|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333|       S|    Mrs|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708|       C|    Mrs|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|       S|   Miss|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55|       S|   Miss|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05|       S|     Mr|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275|       S|     Mr|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542|       S|   Miss|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0|       S|    Mrs|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125|       Q| Master|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0|       S|     Mr|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0|       S|    Mrs|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225|       C|    Mrs|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23b88a33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:49.503504Z",
     "iopub.status.busy": "2022-08-02T09:25:49.502982Z",
     "iopub.status.idle": "2022-08-02T09:25:49.750349Z",
     "shell.execute_reply": "2022-08-02T09:25:49.749006Z"
    },
    "papermill": {
     "duration": 0.332168,
     "end_time": "2022-08-02T09:25:49.753792",
     "exception": false,
     "start_time": "2022-08-02T09:25:49.421624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "| Initial|\n",
      "+--------+\n",
      "|     Don|\n",
      "|    Miss|\n",
      "|Countess|\n",
      "|     Col|\n",
      "|     Rev|\n",
      "|    Lady|\n",
      "|  Master|\n",
      "|     Mme|\n",
      "|    Capt|\n",
      "|      Mr|\n",
      "|      Dr|\n",
      "|     Mrs|\n",
      "|     Sir|\n",
      "|Jonkheer|\n",
      "|    Mlle|\n",
      "|   Major|\n",
      "|      Ms|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.select(\"Initial\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13993a56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:49.857755Z",
     "iopub.status.busy": "2022-08-02T09:25:49.857196Z",
     "iopub.status.idle": "2022-08-02T09:25:49.915101Z",
     "shell.execute_reply": "2022-08-02T09:25:49.913790Z"
    },
    "papermill": {
     "duration": 0.111904,
     "end_time": "2022-08-02T09:25:49.918713",
     "exception": false,
     "start_time": "2022-08-02T09:25:49.806809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can see some misspelled initials like Mlle or Mme that stand for Miss. Let's replace them with Miss and same thing for other values\n",
    "df_train = df_train.replace(['Mlle','Mme', 'Ms', 'Dr','Major','Dona','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "               ['Miss','Miss','Miss','Mr','Mr',  'Mrs','Mrs', 'Mrs',  'Other',  'Other','Other','Mr','Mr','Mr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e4cb480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:50.031106Z",
     "iopub.status.busy": "2022-08-02T09:25:50.030709Z",
     "iopub.status.idle": "2022-08-02T09:25:50.302266Z",
     "shell.execute_reply": "2022-08-02T09:25:50.301054Z"
    },
    "papermill": {
     "duration": 0.323979,
     "end_time": "2022-08-02T09:25:50.305597",
     "exception": false,
     "start_time": "2022-08-02T09:25:49.981618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Initial|\n",
      "+-------+\n",
      "|   Miss|\n",
      "|  Other|\n",
      "| Master|\n",
      "|     Mr|\n",
      "|    Mrs|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.select(\"Initial\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c834c1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:50.411517Z",
     "iopub.status.busy": "2022-08-02T09:25:50.411116Z",
     "iopub.status.idle": "2022-08-02T09:25:50.703619Z",
     "shell.execute_reply": "2022-08-02T09:25:50.702391Z"
    },
    "papermill": {
     "duration": 0.344967,
     "end_time": "2022-08-02T09:25:50.706743",
     "exception": false,
     "start_time": "2022-08-02T09:25:50.361776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Initial='Miss', avg(Age)=21.86),\n",
       " Row(Initial='Other', avg(Age)=45.888888888888886),\n",
       " Row(Initial='Master', avg(Age)=4.574166666666667),\n",
       " Row(Initial='Mr', avg(Age)=32.73960880195599),\n",
       " Row(Initial='Mrs', avg(Age)=35.981818181818184)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('Initial').avg('Age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a91ee3cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:50.854721Z",
     "iopub.status.busy": "2022-08-02T09:25:50.853814Z",
     "iopub.status.idle": "2022-08-02T09:25:50.974991Z",
     "shell.execute_reply": "2022-08-02T09:25:50.973776Z"
    },
    "papermill": {
     "duration": 0.197545,
     "end_time": "2022-08-02T09:25:50.978392",
     "exception": false,
     "start_time": "2022-08-02T09:25:50.780847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_train.withColumn(\"Age\",when((df_train[\"Initial\"] == \"Miss\") & (df_train[\"Age\"].isNull()), 22).otherwise(df_train[\"Age\"]))\n",
    "df_train = df_train.withColumn(\"Age\",when((df_train[\"Initial\"] == \"Other\") & (df_train[\"Age\"].isNull()), 44).otherwise(df_train[\"Age\"]))\n",
    "df_train = df_train.withColumn(\"Age\",when((df_train[\"Initial\"] == \"Master\") & (df_train[\"Age\"].isNull()), 5).otherwise(df_train[\"Age\"]))\n",
    "df_train = df_train.withColumn(\"Age\",when((df_train[\"Initial\"] == \"Mr\") & (df_train[\"Age\"].isNull()), 33).otherwise(df_train[\"Age\"]))\n",
    "df_train = df_train.withColumn(\"Age\",when((df_train[\"Initial\"] == \"Mrs\") & (df_train[\"Age\"].isNull()), 37).otherwise(df_train[\"Age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c76aa44d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:51.083406Z",
     "iopub.status.busy": "2022-08-02T09:25:51.082115Z",
     "iopub.status.idle": "2022-08-02T09:25:51.485732Z",
     "shell.execute_reply": "2022-08-02T09:25:51.484450Z"
    },
    "papermill": {
     "duration": 0.45893,
     "end_time": "2022-08-02T09:25:51.491548",
     "exception": false,
     "start_time": "2022-08-02T09:25:51.032618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|Initial|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|       S|     Mr|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|       C|    Mrs|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|       S|   Miss|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|       S|    Mrs|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|       S|     Mr|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|33.0|    0|    0|          330877| 8.4583|       Q|     Mr|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|       S|     Mr|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075|       S| Master|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333|       S|    Mrs|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708|       C|    Mrs|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|       S|   Miss|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55|       S|   Miss|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05|       S|     Mr|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275|       S|     Mr|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542|       S|   Miss|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0|       S|    Mrs|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125|       Q| Master|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|33.0|    0|    0|          244373|   13.0|       S|     Mr|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0|       S|    Mrs|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|37.0|    0|    0|            2649|  7.225|       C|    Mrs|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b3dd8",
   "metadata": {
    "papermill": {
     "duration": 0.048215,
     "end_time": "2022-08-02T09:25:51.601782",
     "exception": false,
     "start_time": "2022-08-02T09:25:51.553567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Creating a new feature called \"Family_size\",which is the summation of Parch(parents/children) and SibSp(siblings/spouses) and \"Alone\" when family size=0  and analyse it. \n",
    "This enables us to check if survival rate has anything to do with family size of the passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b620a982",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:51.700516Z",
     "iopub.status.busy": "2022-08-02T09:25:51.700069Z",
     "iopub.status.idle": "2022-08-02T09:25:51.983139Z",
     "shell.execute_reply": "2022-08-02T09:25:51.981880Z"
    },
    "papermill": {
     "duration": 0.337013,
     "end_time": "2022-08-02T09:25:51.987187",
     "exception": false,
     "start_time": "2022-08-02T09:25:51.650174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|Family_Size|count|\n",
      "+-----------+-----+\n",
      "|          1|  161|\n",
      "|          6|   12|\n",
      "|          3|   29|\n",
      "|          5|   22|\n",
      "|          4|   15|\n",
      "|          7|    6|\n",
      "|         10|    7|\n",
      "|          2|  102|\n",
      "|          0|  537|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.withColumn(\"Family_Size\",col('SibSp')+col('Parch'))\n",
    "df_train.groupBy(\"Family_Size\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2946d198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:52.095629Z",
     "iopub.status.busy": "2022-08-02T09:25:52.094795Z",
     "iopub.status.idle": "2022-08-02T09:25:52.120546Z",
     "shell.execute_reply": "2022-08-02T09:25:52.119239Z"
    },
    "papermill": {
     "duration": 0.079473,
     "end_time": "2022-08-02T09:25:52.124071",
     "exception": false,
     "start_time": "2022-08-02T09:25:52.044598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 'Alone'=1 means person was alone\n",
    "df_train = df_train.withColumn('Alone',lit(0))\n",
    "df_train = df_train.withColumn(\"Alone\",when(df_train[\"Family_Size\"] == 0, 1).otherwise(df_train[\"Alone\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a108e1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:52.224941Z",
     "iopub.status.busy": "2022-08-02T09:25:52.224457Z",
     "iopub.status.idle": "2022-08-02T09:25:52.556370Z",
     "shell.execute_reply": "2022-08-02T09:25:52.555385Z"
    },
    "papermill": {
     "duration": 0.385051,
     "end_time": "2022-08-02T09:25:52.559090",
     "exception": false,
     "start_time": "2022-08-02T09:25:52.174039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+-----------+-----+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|Initial|Family_Size|Alone|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+-----------+-----+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|       S|     Mr|          1|    0|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|       C|    Mrs|          1|    0|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|       S|   Miss|          0|    1|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|       S|    Mrs|          1|    0|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|       S|     Mr|          0|    1|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|33.0|    0|    0|          330877| 8.4583|       Q|     Mr|          0|    1|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|       S|     Mr|          0|    1|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075|       S| Master|          4|    0|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333|       S|    Mrs|          2|    0|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708|       C|    Mrs|          1|    0|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|       S|   Miss|          2|    0|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55|       S|   Miss|          0|    1|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05|       S|     Mr|          0|    1|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275|       S|     Mr|          6|    0|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542|       S|   Miss|          0|    1|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0|       S|    Mrs|          0|    1|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125|       Q| Master|          5|    0|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|33.0|    0|    0|          244373|   13.0|       S|     Mr|          0|    1|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0|       S|    Mrs|          1|    0|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|37.0|    0|    0|            2649|  7.225|       C|    Mrs|          0|    1|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da3f01cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:52.658543Z",
     "iopub.status.busy": "2022-08-02T09:25:52.658137Z",
     "iopub.status.idle": "2022-08-02T09:25:54.416649Z",
     "shell.execute_reply": "2022-08-02T09:25:54.415200Z"
    },
    "papermill": {
     "duration": 1.81198,
     "end_time": "2022-08-02T09:25:54.420265",
     "exception": false,
     "start_time": "2022-08-02T09:25:52.608285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#converting Sex, Embarked & Initial columns from string to number using StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df_train) for column in [\"Sex\",\"Embarked\",\"Initial\"]]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_train = pipeline.fit(df_train).transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97b9ea3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:54.556950Z",
     "iopub.status.busy": "2022-08-02T09:25:54.555510Z",
     "iopub.status.idle": "2022-08-02T09:25:55.018929Z",
     "shell.execute_reply": "2022-08-02T09:25:55.017775Z"
    },
    "papermill": {
     "duration": 0.529122,
     "end_time": "2022-08-02T09:25:55.022285",
     "exception": false,
     "start_time": "2022-08-02T09:25:54.493163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+-----------+-----+---------+--------------+-------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|Initial|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+-----------+-----+---------+--------------+-------------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|       S|     Mr|          1|    0|      0.0|           0.0|          0.0|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|       C|    Mrs|          1|    0|      1.0|           1.0|          2.0|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|       S|   Miss|          0|    1|      1.0|           0.0|          1.0|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|       S|    Mrs|          1|    0|      1.0|           0.0|          2.0|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|       S|     Mr|          0|    1|      0.0|           0.0|          0.0|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|33.0|    0|    0|          330877| 8.4583|       Q|     Mr|          0|    1|      0.0|           2.0|          0.0|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|       S|     Mr|          0|    1|      0.0|           0.0|          0.0|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075|       S| Master|          4|    0|      0.0|           0.0|          3.0|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333|       S|    Mrs|          2|    0|      1.0|           0.0|          2.0|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708|       C|    Mrs|          1|    0|      1.0|           1.0|          2.0|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|       S|   Miss|          2|    0|      1.0|           0.0|          1.0|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55|       S|   Miss|          0|    1|      1.0|           0.0|          1.0|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05|       S|     Mr|          0|    1|      0.0|           0.0|          0.0|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275|       S|     Mr|          6|    0|      0.0|           0.0|          0.0|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542|       S|   Miss|          0|    1|      1.0|           0.0|          1.0|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0|       S|    Mrs|          0|    1|      1.0|           0.0|          2.0|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125|       Q| Master|          5|    0|      0.0|           2.0|          3.0|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|33.0|    0|    0|          244373|   13.0|       S|     Mr|          0|    1|      0.0|           0.0|          0.0|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0|       S|    Mrs|          1|    0|      1.0|           0.0|          2.0|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|37.0|    0|    0|            2649|  7.225|       C|    Mrs|          0|    1|      1.0|           1.0|          2.0|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------+-----------+-----+---------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53c46c73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:55.144749Z",
     "iopub.status.busy": "2022-08-02T09:25:55.144304Z",
     "iopub.status.idle": "2022-08-02T09:25:55.157945Z",
     "shell.execute_reply": "2022-08-02T09:25:55.156905Z"
    },
    "papermill": {
     "duration": 0.067436,
     "end_time": "2022-08-02T09:25:55.160658",
     "exception": false,
     "start_time": "2022-08-02T09:25:55.093222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_train.drop(\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\",\"Embarked\",\"Sex\",\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "682a95ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:55.262709Z",
     "iopub.status.busy": "2022-08-02T09:25:55.261955Z",
     "iopub.status.idle": "2022-08-02T09:25:55.517910Z",
     "shell.execute_reply": "2022-08-02T09:25:55.516833Z"
    },
    "papermill": {
     "duration": 0.310917,
     "end_time": "2022-08-02T09:25:55.521772",
     "exception": false,
     "start_time": "2022-08-02T09:25:55.210855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|       0|     3|22.0|    1|    0|   7.25|          1|    0|      0.0|           0.0|          0.0|\n",
      "|       1|     1|38.0|    1|    0|71.2833|          1|    0|      1.0|           1.0|          2.0|\n",
      "|       1|     3|26.0|    0|    0|  7.925|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       1|     1|35.0|    1|    0|   53.1|          1|    0|      1.0|           0.0|          2.0|\n",
      "|       0|     3|35.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|33.0|    0|    0| 8.4583|          0|    1|      0.0|           2.0|          0.0|\n",
      "|       0|     1|54.0|    0|    0|51.8625|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3| 2.0|    3|    1| 21.075|          4|    0|      0.0|           0.0|          3.0|\n",
      "|       1|     3|27.0|    0|    2|11.1333|          2|    0|      1.0|           0.0|          2.0|\n",
      "|       1|     2|14.0|    1|    0|30.0708|          1|    0|      1.0|           1.0|          2.0|\n",
      "|       1|     3| 4.0|    1|    1|   16.7|          2|    0|      1.0|           0.0|          1.0|\n",
      "|       1|     1|58.0|    0|    0|  26.55|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       0|     3|20.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|39.0|    1|    5| 31.275|          6|    0|      0.0|           0.0|          0.0|\n",
      "|       0|     3|14.0|    0|    0| 7.8542|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       1|     2|55.0|    0|    0|   16.0|          0|    1|      1.0|           0.0|          2.0|\n",
      "|       0|     3| 2.0|    4|    1| 29.125|          5|    0|      0.0|           2.0|          3.0|\n",
      "|       1|     2|33.0|    0|    0|   13.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|31.0|    1|    0|   18.0|          1|    0|      1.0|           0.0|          2.0|\n",
      "|       1|     3|37.0|    0|    0|  7.225|          0|    1|      1.0|           1.0|          2.0|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4ebad",
   "metadata": {
    "papermill": {
     "duration": 0.049142,
     "end_time": "2022-08-02T09:25:55.621389",
     "exception": false,
     "start_time": "2022-08-02T09:25:55.572247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from output of describe function it is seen that minimum value of the column is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b25310a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:55.722908Z",
     "iopub.status.busy": "2022-08-02T09:25:55.721590Z",
     "iopub.status.idle": "2022-08-02T09:25:56.076217Z",
     "shell.execute_reply": "2022-08-02T09:25:56.075054Z"
    },
    "papermill": {
     "duration": 0.40899,
     "end_time": "2022-08-02T09:25:56.079742",
     "exception": false,
     "start_time": "2022-08-02T09:25:55.670752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+----+-----------+-----+---------+--------------+-------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+--------+------+----+-----+-----+----+-----------+-----+---------+--------------+-------------+\n",
      "|       0|     3|36.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     1|40.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       1|     3|25.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     2|33.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|19.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     2|33.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     2|33.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     2|33.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|49.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     1|33.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     2|33.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     2|33.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     1|39.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     1|33.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     1|38.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          4.0|\n",
      "+--------+------+----+-----+-----+----+-----------+-----+---------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.filter(df_train[\"Fare\"]==0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a243bb4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:56.196410Z",
     "iopub.status.busy": "2022-08-02T09:25:56.195271Z",
     "iopub.status.idle": "2022-08-02T09:25:56.420271Z",
     "shell.execute_reply": "2022-08-02T09:25:56.418954Z"
    },
    "papermill": {
     "duration": 0.278885,
     "end_time": "2022-08-02T09:25:56.423298",
     "exception": false,
     "start_time": "2022-08-02T09:25:56.144413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Pclass=1, avg(Fare)=84.15468749999992),\n",
       " Row(Pclass=3, avg(Fare)=13.675550101832997),\n",
       " Row(Pclass=2, avg(Fare)=20.66218315217391)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the average fares per class\n",
    "df_train.groupby('Pclass').avg('Fare').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5e0ab41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:56.545389Z",
     "iopub.status.busy": "2022-08-02T09:25:56.544495Z",
     "iopub.status.idle": "2022-08-02T09:25:56.598269Z",
     "shell.execute_reply": "2022-08-02T09:25:56.596822Z"
    },
    "papermill": {
     "duration": 0.108895,
     "end_time": "2022-08-02T09:25:56.601293",
     "exception": false,
     "start_time": "2022-08-02T09:25:56.492398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill blank fare\n",
    "df_train = df_train.withColumn(\"Fare\",when((df_train[\"Pclass\"] == 1) & (df_train[\"Fare\"]==0), 84).otherwise(df_train[\"Fare\"]))\n",
    "df_train = df_train.withColumn(\"Fare\",when((df_train[\"Pclass\"] == 2) & (df_train[\"Fare\"]==0), 20).otherwise(df_train[\"Fare\"]))\n",
    "df_train = df_train.withColumn(\"Fare\",when((df_train[\"Pclass\"] == 3) & (df_train[\"Fare\"]==0), 13).otherwise(df_train[\"Fare\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "319fad30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:56.704434Z",
     "iopub.status.busy": "2022-08-02T09:25:56.703707Z",
     "iopub.status.idle": "2022-08-02T09:25:57.042504Z",
     "shell.execute_reply": "2022-08-02T09:25:57.041474Z"
    },
    "papermill": {
     "duration": 0.393804,
     "end_time": "2022-08-02T09:25:57.046309",
     "exception": false,
     "start_time": "2022-08-02T09:25:56.652505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|       0|     3|22.0|    1|    0|   7.25|          1|    0|      0.0|           0.0|          0.0|\n",
      "|       1|     1|38.0|    1|    0|71.2833|          1|    0|      1.0|           1.0|          2.0|\n",
      "|       1|     3|26.0|    0|    0|  7.925|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       1|     1|35.0|    1|    0|   53.1|          1|    0|      1.0|           0.0|          2.0|\n",
      "|       0|     3|35.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|33.0|    0|    0| 8.4583|          0|    1|      0.0|           2.0|          0.0|\n",
      "|       0|     1|54.0|    0|    0|51.8625|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3| 2.0|    3|    1| 21.075|          4|    0|      0.0|           0.0|          3.0|\n",
      "|       1|     3|27.0|    0|    2|11.1333|          2|    0|      1.0|           0.0|          2.0|\n",
      "|       1|     2|14.0|    1|    0|30.0708|          1|    0|      1.0|           1.0|          2.0|\n",
      "|       1|     3| 4.0|    1|    1|   16.7|          2|    0|      1.0|           0.0|          1.0|\n",
      "|       1|     1|58.0|    0|    0|  26.55|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       0|     3|20.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|39.0|    1|    5| 31.275|          6|    0|      0.0|           0.0|          0.0|\n",
      "|       0|     3|14.0|    0|    0| 7.8542|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       1|     2|55.0|    0|    0|   16.0|          0|    1|      1.0|           0.0|          2.0|\n",
      "|       0|     3| 2.0|    4|    1| 29.125|          5|    0|      0.0|           2.0|          3.0|\n",
      "|       1|     2|33.0|    0|    0|   13.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|31.0|    1|    0|   18.0|          1|    0|      1.0|           0.0|          2.0|\n",
      "|       1|     3|37.0|    0|    0|  7.225|          0|    1|      1.0|           1.0|          2.0|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac87f2bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:57.151785Z",
     "iopub.status.busy": "2022-08-02T09:25:57.151316Z",
     "iopub.status.idle": "2022-08-02T09:25:57.530014Z",
     "shell.execute_reply": "2022-08-02T09:25:57.527992Z"
    },
    "papermill": {
     "duration": 0.436085,
     "end_time": "2022-08-02T09:25:57.534726",
     "exception": false,
     "start_time": "2022-08-02T09:25:57.098641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+-----+-----+----+-----------+-----+---------+--------------+-------------+\n",
      "|Survived|Pclass|Age|SibSp|Parch|Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+--------+------+---+-----+-----+----+-----------+-----+---------+--------------+-------------+\n",
      "+--------+------+---+-----+-----+----+-----------+-----+---------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.filter(df_train[\"Fare\"]==0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2dfe1e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:57.647415Z",
     "iopub.status.busy": "2022-08-02T09:25:57.646206Z",
     "iopub.status.idle": "2022-08-02T09:25:57.780517Z",
     "shell.execute_reply": "2022-08-02T09:25:57.779423Z"
    },
    "papermill": {
     "duration": 0.188681,
     "end_time": "2022-08-02T09:25:57.783297",
     "exception": false,
     "start_time": "2022-08-02T09:25:57.594616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Putting all the features into a vector\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature = VectorAssembler(inputCols=df_train.columns[1:],outputCol=\"features\")#leving the first column\n",
    "feature_vector= feature.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae8410c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:57.886524Z",
     "iopub.status.busy": "2022-08-02T09:25:57.885812Z",
     "iopub.status.idle": "2022-08-02T09:25:57.892473Z",
     "shell.execute_reply": "2022-08-02T09:25:57.891654Z"
    },
    "papermill": {
     "duration": 0.060826,
     "end_time": "2022-08-02T09:25:57.894648",
     "exception": false,
     "start_time": "2022-08-02T09:25:57.833822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_14c67ca3b7a9"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3de821b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:57.999820Z",
     "iopub.status.busy": "2022-08-02T09:25:57.998661Z",
     "iopub.status.idle": "2022-08-02T09:25:58.550599Z",
     "shell.execute_reply": "2022-08-02T09:25:58.549713Z"
    },
    "papermill": {
     "duration": 0.607585,
     "end_time": "2022-08-02T09:25:58.553794",
     "exception": false,
     "start_time": "2022-08-02T09:25:57.946209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|            features|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "|       0|     3|22.0|    1|    0|   7.25|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,2,4,5],[...|\n",
      "|       1|     1|38.0|    1|    0|71.2833|          1|    0|      1.0|           1.0|          2.0|[1.0,38.0,1.0,0.0...|\n",
      "|       1|     3|26.0|    0|    0|  7.925|          0|    1|      1.0|           0.0|          1.0|[3.0,26.0,0.0,0.0...|\n",
      "|       1|     1|35.0|    1|    0|   53.1|          1|    0|      1.0|           0.0|          2.0|[1.0,35.0,1.0,0.0...|\n",
      "|       0|     3|35.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[3....|\n",
      "|       0|     3|33.0|    0|    0| 8.4583|          0|    1|      0.0|           2.0|          0.0|(10,[0,1,4,6,8],[...|\n",
      "|       0|     1|54.0|    0|    0|51.8625|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     3| 2.0|    3|    1| 21.075|          4|    0|      0.0|           0.0|          3.0|[3.0,2.0,3.0,1.0,...|\n",
      "|       1|     3|27.0|    0|    2|11.1333|          2|    0|      1.0|           0.0|          2.0|[3.0,27.0,0.0,2.0...|\n",
      "|       1|     2|14.0|    1|    0|30.0708|          1|    0|      1.0|           1.0|          2.0|[2.0,14.0,1.0,0.0...|\n",
      "|       1|     3| 4.0|    1|    1|   16.7|          2|    0|      1.0|           0.0|          1.0|[3.0,4.0,1.0,1.0,...|\n",
      "|       1|     1|58.0|    0|    0|  26.55|          0|    1|      1.0|           0.0|          1.0|[1.0,58.0,0.0,0.0...|\n",
      "|       0|     3|20.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[3....|\n",
      "|       0|     3|39.0|    1|    5| 31.275|          6|    0|      0.0|           0.0|          0.0|[3.0,39.0,1.0,5.0...|\n",
      "|       0|     3|14.0|    0|    0| 7.8542|          0|    1|      1.0|           0.0|          1.0|[3.0,14.0,0.0,0.0...|\n",
      "|       1|     2|55.0|    0|    0|   16.0|          0|    1|      1.0|           0.0|          2.0|[2.0,55.0,0.0,0.0...|\n",
      "|       0|     3| 2.0|    4|    1| 29.125|          5|    0|      0.0|           2.0|          3.0|[3.0,2.0,4.0,1.0,...|\n",
      "|       1|     2|33.0|    0|    0|   13.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[2....|\n",
      "|       0|     3|31.0|    1|    0|   18.0|          1|    0|      1.0|           0.0|          2.0|[3.0,31.0,1.0,0.0...|\n",
      "|       1|     3|37.0|    0|    0|  7.225|          0|    1|      1.0|           1.0|          2.0|[3.0,37.0,0.0,0.0...|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff2084dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:58.672435Z",
     "iopub.status.busy": "2022-08-02T09:25:58.671959Z",
     "iopub.status.idle": "2022-08-02T09:25:58.914888Z",
     "shell.execute_reply": "2022-08-02T09:25:58.913492Z"
    },
    "papermill": {
     "duration": 0.298749,
     "end_time": "2022-08-02T09:25:58.917853",
     "exception": false,
     "start_time": "2022-08-02T09:25:58.619104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(trainingData, validData) = feature_vector.randomSplit([0.8, 0.2],seed = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77a0a8f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:59.022349Z",
     "iopub.status.busy": "2022-08-02T09:25:59.021277Z",
     "iopub.status.idle": "2022-08-02T09:25:59.572977Z",
     "shell.execute_reply": "2022-08-02T09:25:59.571118Z"
    },
    "papermill": {
     "duration": 0.607157,
     "end_time": "2022-08-02T09:25:59.576253",
     "exception": false,
     "start_time": "2022-08-02T09:25:58.969096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|    Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|            features|\n",
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "|       0|     1| 2.0|    1|    2|  151.55|          3|    0|      1.0|           0.0|          1.0|[1.0,2.0,1.0,2.0,...|\n",
      "|       0|     1|18.0|    1|    0|   108.9|          1|    0|      0.0|           1.0|          0.0|[1.0,18.0,1.0,0.0...|\n",
      "|       0|     1|19.0|    1|    0|    53.1|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,2,4,5],[...|\n",
      "|       0|     1|19.0|    3|    2|   263.0|          5|    0|      0.0|           0.0|          0.0|[1.0,19.0,3.0,2.0...|\n",
      "|       0|     1|21.0|    0|    1| 77.2875|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,3,4,5],[...|\n",
      "|       0|     1|24.0|    0|    1|247.5208|          1|    0|      0.0|           1.0|          0.0|[1.0,24.0,0.0,1.0...|\n",
      "|       0|     1|25.0|    1|    2|  151.55|          3|    0|      1.0|           0.0|          2.0|[1.0,25.0,1.0,2.0...|\n",
      "|       0|     1|27.0|    0|    2|   211.5|          2|    0|      0.0|           1.0|          0.0|[1.0,27.0,0.0,2.0...|\n",
      "|       0|     1|28.0|    0|    0|    47.1|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|28.0|    1|    0| 82.1708|          1|    0|      0.0|           1.0|          0.0|[1.0,28.0,1.0,0.0...|\n",
      "|       0|     1|31.0|    0|    0| 50.4958|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|33.0|    0|    0|  25.925|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|33.0|    0|    0|    26.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|33.0|    0|    0|   26.55|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|33.0|    0|    0| 27.7208|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|\n",
      "|       0|     1|33.0|    0|    0| 30.6958|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|\n",
      "|       0|     1|33.0|    0|    0|    31.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|33.0|    0|    0|    35.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|33.0|    0|    0|    39.6|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|\n",
      "|       0|     1|33.0|    0|    0|    50.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0eb32dd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:25:59.728426Z",
     "iopub.status.busy": "2022-08-02T09:25:59.727873Z",
     "iopub.status.idle": "2022-08-02T09:26:00.153425Z",
     "shell.execute_reply": "2022-08-02T09:26:00.151384Z"
    },
    "papermill": {
     "duration": 0.505377,
     "end_time": "2022-08-02T09:26:00.157266",
     "exception": false,
     "start_time": "2022-08-02T09:25:59.651889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|    Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|            features|\n",
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "|       0|     1|22.0|    0|    0|135.6333|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|\n",
      "|       0|     1|24.0|    0|    0|    79.2|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|\n",
      "|       0|     1|29.0|    0|    0|    30.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|29.0|    1|    0|    66.6|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,2,4,5],[...|\n",
      "|       0|     1|30.0|    0|    0|   27.75|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|\n",
      "|       0|     1|31.0|    1|    0|    52.0|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,2,4,5],[...|\n",
      "|       0|     1|33.0|    0|    0|     5.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|33.0|    0|    0| 27.7208|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|\n",
      "|       0|     1|33.0|    0|    0|    42.4|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|33.0|    0|    0|    52.0|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|33.0|    0|    0|221.7792|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|37.0|    1|    0|    53.1|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,2,4,5],[...|\n",
      "|       0|     1|38.0|    0|    1|153.4625|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,3,4,5],[...|\n",
      "|       0|     1|45.0|    1|    0|  83.475|          1|    0|      0.0|           0.0|          0.0|(10,[0,1,2,4,5],[...|\n",
      "|       0|     1|47.0|    0|    0| 25.5875|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|58.0|    0|    2| 113.275|          2|    0|      0.0|           1.0|          0.0|[1.0,58.0,0.0,2.0...|\n",
      "|       0|     1|62.0|    0|    0|   26.55|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|65.0|    0|    0|   26.55|          0|    1|      0.0|           0.0|          0.0|(10,[0,1,4,6],[1....|\n",
      "|       0|     1|71.0|    0|    0| 34.6542|          0|    1|      0.0|           1.0|          0.0|(10,[0,1,4,6,8],[...|\n",
      "|       0|     2|19.0|    1|    1|   36.75|          2|    0|      0.0|           0.0|          0.0|[2.0,19.0,1.0,1.0...|\n",
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59c020",
   "metadata": {
    "papermill": {
     "duration": 0.051153,
     "end_time": "2022-08-02T09:26:00.271898",
     "exception": false,
     "start_time": "2022-08-02T09:26:00.220745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**MODELLING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "848a4b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:00.375743Z",
     "iopub.status.busy": "2022-08-02T09:26:00.375299Z",
     "iopub.status.idle": "2022-08-02T09:26:04.204253Z",
     "shell.execute_reply": "2022-08-02T09:26:04.202722Z"
    },
    "papermill": {
     "duration": 3.885248,
     "end_time": "2022-08-02T09:26:04.208216",
     "exception": false,
     "start_time": "2022-08-02T09:26:00.322968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       1.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       1.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       1.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,3,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,2,4,5],[...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|[1.0,58.0,0.0,2.0...|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|(10,[0,1,4,6],[1....|\n",
      "|       0.0|       0|(10,[0,1,4,6,8],[...|\n",
      "|       0.0|       0|[2.0,19.0,1.0,1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "\n",
    "# Training our Model\n",
    "lrModel = lr.fit(trainingData)\n",
    "lr_prediction = lrModel.transform(validData)\n",
    "lr_prediction.select(\"prediction\", \"Survived\", \"features\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a466152f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:04.346600Z",
     "iopub.status.busy": "2022-08-02T09:26:04.345814Z",
     "iopub.status.idle": "2022-08-02T09:26:04.867995Z",
     "shell.execute_reply": "2022-08-02T09:26:04.866765Z"
    },
    "papermill": {
     "duration": 0.584416,
     "end_time": "2022-08-02T09:26:04.871257",
     "exception": false,
     "start_time": "2022-08-02T09:26:04.286841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of LogisticRegression is = 0.781915\n",
      "Validation Error of LogisticRegression = 0.218085 \n"
     ]
    }
   ],
   "source": [
    "lr_accuracy = evaluator.evaluate(lr_prediction)\n",
    "print(\"Validation Accuracy of LogisticRegression is = %g\"% (lr_accuracy))\n",
    "print(\"Validation Error of LogisticRegression = %g \" % (1.0 - lr_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e31dda02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:05.012420Z",
     "iopub.status.busy": "2022-08-02T09:26:05.012015Z",
     "iopub.status.idle": "2022-08-02T09:26:05.130828Z",
     "shell.execute_reply": "2022-08-02T09:26:05.129427Z"
    },
    "papermill": {
     "duration": 0.187599,
     "end_time": "2022-08-02T09:26:05.135928",
     "exception": false,
     "start_time": "2022-08-02T09:26:04.948329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|        892|     3|    Kelly, Mr. James|  male|34.5|    0|    0|          330911| 7.8292| null|       Q|\n",
      "|        893|     3|Wilkes, Mrs. Jame...|female|47.0|    1|    0|          363272|    7.0| null|       S|\n",
      "|        894|     2|Myles, Mr. Thomas...|  male|62.0|    0|    0|          240276| 9.6875| null|       Q|\n",
      "|        895|     3|    Wirz, Mr. Albert|  male|27.0|    0|    0|          315154| 8.6625| null|       S|\n",
      "|        896|     3|Hirvonen, Mrs. Al...|female|22.0|    1|    1|         3101298|12.2875| null|       S|\n",
      "|        897|     3|Svensson, Mr. Joh...|  male|14.0|    0|    0|            7538|  9.225| null|       S|\n",
      "|        898|     3|Connolly, Miss. Kate|female|30.0|    0|    0|          330972| 7.6292| null|       Q|\n",
      "|        899|     2|Caldwell, Mr. Alb...|  male|26.0|    1|    1|          248738|   29.0| null|       S|\n",
      "|        900|     3|Abrahim, Mrs. Jos...|female|18.0|    0|    0|            2657| 7.2292| null|       C|\n",
      "|        901|     3|Davies, Mr. John ...|  male|21.0|    2|    0|       A/4 48871|  24.15| null|       S|\n",
      "|        902|     3|    Ilieff, Mr. Ylio|  male|null|    0|    0|          349220| 7.8958| null|       S|\n",
      "|        903|     1|Jones, Mr. Charle...|  male|46.0|    0|    0|             694|   26.0| null|       S|\n",
      "|        904|     1|Snyder, Mrs. John...|female|23.0|    1|    0|           21228|82.2667|  B45|       S|\n",
      "|        905|     2|Howard, Mr. Benjamin|  male|63.0|    1|    0|           24065|   26.0| null|       S|\n",
      "|        906|     1|Chaffee, Mrs. Her...|female|47.0|    1|    0|     W.E.P. 5734| 61.175|  E31|       S|\n",
      "|        907|     2|del Carlo, Mrs. S...|female|24.0|    1|    0|   SC/PARIS 2167|27.7208| null|       C|\n",
      "|        908|     2|   Keane, Mr. Daniel|  male|35.0|    0|    0|          233734|  12.35| null|       Q|\n",
      "|        909|     3|   Assaf, Mr. Gerios|  male|21.0|    0|    0|            2692|  7.225| null|       C|\n",
      "|        910|     3|Ilmakangas, Miss....|female|27.0|    1|    0|STON/O2. 3101270|  7.925| null|       S|\n",
      "|        911|     3|\"Assaf Khalil, Mr...|female|45.0|    0|    0|            2696|  7.225| null|       C|\n",
      "+-----------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2eab11c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:05.267967Z",
     "iopub.status.busy": "2022-08-02T09:26:05.266471Z",
     "iopub.status.idle": "2022-08-02T09:26:05.461203Z",
     "shell.execute_reply": "2022-08-02T09:26:05.459558Z"
    },
    "papermill": {
     "duration": 0.252857,
     "end_time": "2022-08-02T09:26:05.464881",
     "exception": false,
     "start_time": "2022-08-02T09:26:05.212024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|       0|     3|22.0|    1|    0|   7.25|          1|    0|      0.0|           0.0|          0.0|\n",
      "|       1|     1|38.0|    1|    0|71.2833|          1|    0|      1.0|           1.0|          2.0|\n",
      "|       1|     3|26.0|    0|    0|  7.925|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       1|     1|35.0|    1|    0|   53.1|          1|    0|      1.0|           0.0|          2.0|\n",
      "|       0|     3|35.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|33.0|    0|    0| 8.4583|          0|    1|      0.0|           2.0|          0.0|\n",
      "|       0|     1|54.0|    0|    0|51.8625|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3| 2.0|    3|    1| 21.075|          4|    0|      0.0|           0.0|          3.0|\n",
      "|       1|     3|27.0|    0|    2|11.1333|          2|    0|      1.0|           0.0|          2.0|\n",
      "|       1|     2|14.0|    1|    0|30.0708|          1|    0|      1.0|           1.0|          2.0|\n",
      "|       1|     3| 4.0|    1|    1|   16.7|          2|    0|      1.0|           0.0|          1.0|\n",
      "|       1|     1|58.0|    0|    0|  26.55|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       0|     3|20.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|39.0|    1|    5| 31.275|          6|    0|      0.0|           0.0|          0.0|\n",
      "|       0|     3|14.0|    0|    0| 7.8542|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       1|     2|55.0|    0|    0|   16.0|          0|    1|      1.0|           0.0|          2.0|\n",
      "|       0|     3| 2.0|    4|    1| 29.125|          5|    0|      0.0|           2.0|          3.0|\n",
      "|       1|     2|33.0|    0|    0|   13.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|31.0|    1|    0|   18.0|          1|    0|      1.0|           0.0|          2.0|\n",
      "|       1|     3|37.0|    0|    0|  7.225|          0|    1|      1.0|           1.0|          2.0|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610491f",
   "metadata": {
    "papermill": {
     "duration": 0.074364,
     "end_time": "2022-08-02T09:26:05.614880",
     "exception": false,
     "start_time": "2022-08-02T09:26:05.540516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**PREPROCESSING THE TEST DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63355307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:05.719971Z",
     "iopub.status.busy": "2022-08-02T09:26:05.719177Z",
     "iopub.status.idle": "2022-08-02T09:26:05.882709Z",
     "shell.execute_reply": "2022-08-02T09:26:05.881384Z"
    },
    "papermill": {
     "duration": 0.219661,
     "end_time": "2022-08-02T09:26:05.886303",
     "exception": false,
     "start_time": "2022-08-02T09:26:05.666642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|Family_Size|count|\n",
      "+-----------+-----+\n",
      "|          1|   74|\n",
      "|          6|    4|\n",
      "|          3|   14|\n",
      "|          5|    3|\n",
      "|          4|    7|\n",
      "|          7|    2|\n",
      "|         10|    4|\n",
      "|          2|   57|\n",
      "|          0|  253|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.withColumn(\"Family_Size\",col('SibSp')+col('Parch'))\n",
    "df_test.groupBy(\"Family_Size\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f570c964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:06.010542Z",
     "iopub.status.busy": "2022-08-02T09:26:06.010100Z",
     "iopub.status.idle": "2022-08-02T09:26:06.030454Z",
     "shell.execute_reply": "2022-08-02T09:26:06.029480Z"
    },
    "papermill": {
     "duration": 0.076877,
     "end_time": "2022-08-02T09:26:06.032909",
     "exception": false,
     "start_time": "2022-08-02T09:26:05.956032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 'Alone'=1 means person was alone\n",
    "df_test = df_test.withColumn('Alone',lit(0))\n",
    "df_test = df_test.withColumn(\"Alone\",when(df_test[\"Family_Size\"] == 0, 1).otherwise(df_test[\"Alone\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99d4d0a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:06.139151Z",
     "iopub.status.busy": "2022-08-02T09:26:06.138144Z",
     "iopub.status.idle": "2022-08-02T09:26:06.254162Z",
     "shell.execute_reply": "2022-08-02T09:26:06.252477Z"
    },
    "papermill": {
     "duration": 0.172513,
     "end_time": "2022-08-02T09:26:06.257393",
     "exception": false,
     "start_time": "2022-08-02T09:26:06.084880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----------+-----+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|Family_Size|Alone|\n",
      "+-----------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----------+-----+\n",
      "|        892|     3|    Kelly, Mr. James|  male|34.5|    0|    0|          330911| 7.8292| null|       Q|          0|    1|\n",
      "|        893|     3|Wilkes, Mrs. Jame...|female|47.0|    1|    0|          363272|    7.0| null|       S|          1|    0|\n",
      "|        894|     2|Myles, Mr. Thomas...|  male|62.0|    0|    0|          240276| 9.6875| null|       Q|          0|    1|\n",
      "|        895|     3|    Wirz, Mr. Albert|  male|27.0|    0|    0|          315154| 8.6625| null|       S|          0|    1|\n",
      "|        896|     3|Hirvonen, Mrs. Al...|female|22.0|    1|    1|         3101298|12.2875| null|       S|          2|    0|\n",
      "|        897|     3|Svensson, Mr. Joh...|  male|14.0|    0|    0|            7538|  9.225| null|       S|          0|    1|\n",
      "|        898|     3|Connolly, Miss. Kate|female|30.0|    0|    0|          330972| 7.6292| null|       Q|          0|    1|\n",
      "|        899|     2|Caldwell, Mr. Alb...|  male|26.0|    1|    1|          248738|   29.0| null|       S|          2|    0|\n",
      "|        900|     3|Abrahim, Mrs. Jos...|female|18.0|    0|    0|            2657| 7.2292| null|       C|          0|    1|\n",
      "|        901|     3|Davies, Mr. John ...|  male|21.0|    2|    0|       A/4 48871|  24.15| null|       S|          2|    0|\n",
      "|        902|     3|    Ilieff, Mr. Ylio|  male|null|    0|    0|          349220| 7.8958| null|       S|          0|    1|\n",
      "|        903|     1|Jones, Mr. Charle...|  male|46.0|    0|    0|             694|   26.0| null|       S|          0|    1|\n",
      "|        904|     1|Snyder, Mrs. John...|female|23.0|    1|    0|           21228|82.2667|  B45|       S|          1|    0|\n",
      "|        905|     2|Howard, Mr. Benjamin|  male|63.0|    1|    0|           24065|   26.0| null|       S|          1|    0|\n",
      "|        906|     1|Chaffee, Mrs. Her...|female|47.0|    1|    0|     W.E.P. 5734| 61.175|  E31|       S|          1|    0|\n",
      "|        907|     2|del Carlo, Mrs. S...|female|24.0|    1|    0|   SC/PARIS 2167|27.7208| null|       C|          1|    0|\n",
      "|        908|     2|   Keane, Mr. Daniel|  male|35.0|    0|    0|          233734|  12.35| null|       Q|          0|    1|\n",
      "|        909|     3|   Assaf, Mr. Gerios|  male|21.0|    0|    0|            2692|  7.225| null|       C|          0|    1|\n",
      "|        910|     3|Ilmakangas, Miss....|female|27.0|    1|    0|STON/O2. 3101270|  7.925| null|       S|          1|    0|\n",
      "|        911|     3|\"Assaf Khalil, Mr...|female|45.0|    0|    0|            2696|  7.225| null|       C|          0|    1|\n",
      "+-----------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6274a7b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:06.365019Z",
     "iopub.status.busy": "2022-08-02T09:26:06.364603Z",
     "iopub.status.idle": "2022-08-02T09:26:06.380865Z",
     "shell.execute_reply": "2022-08-02T09:26:06.379727Z"
    },
    "papermill": {
     "duration": 0.072634,
     "end_time": "2022-08-02T09:26:06.383684",
     "exception": false,
     "start_time": "2022-08-02T09:26:06.311050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = df_test.withColumn(\"Initial\",regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aeedd4d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:06.490735Z",
     "iopub.status.busy": "2022-08-02T09:26:06.490270Z",
     "iopub.status.idle": "2022-08-02T09:26:06.687076Z",
     "shell.execute_reply": "2022-08-02T09:26:06.685148Z"
    },
    "papermill": {
     "duration": 0.25494,
     "end_time": "2022-08-02T09:26:06.691476",
     "exception": false,
     "start_time": "2022-08-02T09:26:06.436536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Initial|\n",
      "+-------+\n",
      "|   Dona|\n",
      "|   Miss|\n",
      "|    Col|\n",
      "|    Rev|\n",
      "| Master|\n",
      "|     Mr|\n",
      "|     Dr|\n",
      "|    Mrs|\n",
      "|     Ms|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select(\"Initial\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcfb08e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:06.867135Z",
     "iopub.status.busy": "2022-08-02T09:26:06.866604Z",
     "iopub.status.idle": "2022-08-02T09:26:06.924966Z",
     "shell.execute_reply": "2022-08-02T09:26:06.923568Z"
    },
    "papermill": {
     "duration": 0.124787,
     "end_time": "2022-08-02T09:26:06.928127",
     "exception": false,
     "start_time": "2022-08-02T09:26:06.803340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = df_test.replace(['Mlle','Mme', 'Ms', 'Dr','Major','Dona','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "               ['Miss','Miss','Miss','Mr','Mr',  'Mrs','Mrs', 'Mrs',  'Other',  'Other','Other','Mr','Mr','Mr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed9ae82f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:07.049847Z",
     "iopub.status.busy": "2022-08-02T09:26:07.049395Z",
     "iopub.status.idle": "2022-08-02T09:26:07.233597Z",
     "shell.execute_reply": "2022-08-02T09:26:07.231605Z"
    },
    "papermill": {
     "duration": 0.241175,
     "end_time": "2022-08-02T09:26:07.236618",
     "exception": false,
     "start_time": "2022-08-02T09:26:06.995443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Initial|\n",
      "+-------+\n",
      "|   Miss|\n",
      "|  Other|\n",
      "| Master|\n",
      "|     Mr|\n",
      "|    Mrs|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select(\"Initial\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67cee368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:07.371473Z",
     "iopub.status.busy": "2022-08-02T09:26:07.370654Z",
     "iopub.status.idle": "2022-08-02T09:26:07.552913Z",
     "shell.execute_reply": "2022-08-02T09:26:07.551569Z"
    },
    "papermill": {
     "duration": 0.240985,
     "end_time": "2022-08-02T09:26:07.556589",
     "exception": false,
     "start_time": "2022-08-02T09:26:07.315604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Initial='Miss', avg(Age)=21.774843750000002),\n",
       " Row(Initial='Other', avg(Age)=42.75),\n",
       " Row(Initial='Master', avg(Age)=7.406470588235294),\n",
       " Row(Initial='Mr', avg(Age)=32.11413043478261),\n",
       " Row(Initial='Mrs', avg(Age)=38.904761904761905)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby('Initial').avg('Age').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d186b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:07.663664Z",
     "iopub.status.busy": "2022-08-02T09:26:07.663281Z",
     "iopub.status.idle": "2022-08-02T09:26:07.727839Z",
     "shell.execute_reply": "2022-08-02T09:26:07.726499Z"
    },
    "papermill": {
     "duration": 0.121693,
     "end_time": "2022-08-02T09:26:07.731468",
     "exception": false,
     "start_time": "2022-08-02T09:26:07.609775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = df_test.withColumn(\"Age\",when((df_test[\"Initial\"] == \"Miss\") & (df_test[\"Age\"].isNull()), 21).otherwise(df_test[\"Age\"]))\n",
    "df_test = df_test.withColumn(\"Age\",when((df_test[\"Initial\"] == \"Other\") & (df_test[\"Age\"].isNull()), 42).otherwise(df_test[\"Age\"]))\n",
    "df_test = df_test.withColumn(\"Age\",when((df_test[\"Initial\"] == \"Master\") & (df_test[\"Age\"].isNull()), 7).otherwise(df_test[\"Age\"]))\n",
    "df_test = df_test.withColumn(\"Age\",when((df_test[\"Initial\"] == \"Mr\") & (df_test[\"Age\"].isNull()), 32).otherwise(df_test[\"Age\"]))\n",
    "df_test = df_test.withColumn(\"Age\",when((df_test[\"Initial\"] == \"Mrs\") & (df_test[\"Age\"].isNull()), 38).otherwise(df_test[\"Age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c078146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:07.846546Z",
     "iopub.status.busy": "2022-08-02T09:26:07.846158Z",
     "iopub.status.idle": "2022-08-02T09:26:08.609756Z",
     "shell.execute_reply": "2022-08-02T09:26:08.608830Z"
    },
    "papermill": {
     "duration": 0.820567,
     "end_time": "2022-08-02T09:26:08.613080",
     "exception": false,
     "start_time": "2022-08-02T09:26:07.792513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#converting Sex, Embarked & Initial columns from string to number using StringIndexer\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df_test) for column in [\"Sex\",\"Embarked\",\"Initial\"]]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_test = pipeline.fit(df_test).transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46dc1ea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:08.742972Z",
     "iopub.status.busy": "2022-08-02T09:26:08.742552Z",
     "iopub.status.idle": "2022-08-02T09:26:08.753910Z",
     "shell.execute_reply": "2022-08-02T09:26:08.752996Z"
    },
    "papermill": {
     "duration": 0.067198,
     "end_time": "2022-08-02T09:26:08.756283",
     "exception": false,
     "start_time": "2022-08-02T09:26:08.689085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = df_test.drop(\"Name\",\"Ticket\",\"Cabin\",\"Embarked\",\"Sex\",\"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "20172b95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:08.865065Z",
     "iopub.status.busy": "2022-08-02T09:26:08.864639Z",
     "iopub.status.idle": "2022-08-02T09:26:09.053526Z",
     "shell.execute_reply": "2022-08-02T09:26:09.052170Z"
    },
    "papermill": {
     "duration": 0.247081,
     "end_time": "2022-08-02T09:26:09.057400",
     "exception": false,
     "start_time": "2022-08-02T09:26:08.810319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|PassengerId|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+-----------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|        892|     3|34.5|    0|    0| 7.8292|          0|    1|      0.0|           2.0|          0.0|\n",
      "|        893|     3|47.0|    1|    0|    7.0|          1|    0|      1.0|           0.0|          2.0|\n",
      "|        894|     2|62.0|    0|    0| 9.6875|          0|    1|      0.0|           2.0|          0.0|\n",
      "|        895|     3|27.0|    0|    0| 8.6625|          0|    1|      0.0|           0.0|          0.0|\n",
      "|        896|     3|22.0|    1|    1|12.2875|          2|    0|      1.0|           0.0|          2.0|\n",
      "|        897|     3|14.0|    0|    0|  9.225|          0|    1|      0.0|           0.0|          0.0|\n",
      "|        898|     3|30.0|    0|    0| 7.6292|          0|    1|      1.0|           2.0|          1.0|\n",
      "|        899|     2|26.0|    1|    1|   29.0|          2|    0|      0.0|           0.0|          0.0|\n",
      "|        900|     3|18.0|    0|    0| 7.2292|          0|    1|      1.0|           1.0|          2.0|\n",
      "|        901|     3|21.0|    2|    0|  24.15|          2|    0|      0.0|           0.0|          0.0|\n",
      "|        902|     3|32.0|    0|    0| 7.8958|          0|    1|      0.0|           0.0|          0.0|\n",
      "|        903|     1|46.0|    0|    0|   26.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|        904|     1|23.0|    1|    0|82.2667|          1|    0|      1.0|           0.0|          2.0|\n",
      "|        905|     2|63.0|    1|    0|   26.0|          1|    0|      0.0|           0.0|          0.0|\n",
      "|        906|     1|47.0|    1|    0| 61.175|          1|    0|      1.0|           0.0|          2.0|\n",
      "|        907|     2|24.0|    1|    0|27.7208|          1|    0|      1.0|           1.0|          2.0|\n",
      "|        908|     2|35.0|    0|    0|  12.35|          0|    1|      0.0|           2.0|          0.0|\n",
      "|        909|     3|21.0|    0|    0|  7.225|          0|    1|      0.0|           1.0|          0.0|\n",
      "|        910|     3|27.0|    1|    0|  7.925|          1|    0|      1.0|           0.0|          1.0|\n",
      "|        911|     3|45.0|    0|    0|  7.225|          0|    1|      1.0|           1.0|          2.0|\n",
      "+-----------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90fa930b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:09.194021Z",
     "iopub.status.busy": "2022-08-02T09:26:09.192679Z",
     "iopub.status.idle": "2022-08-02T09:26:09.461103Z",
     "shell.execute_reply": "2022-08-02T09:26:09.459777Z"
    },
    "papermill": {
     "duration": 0.33042,
     "end_time": "2022-08-02T09:26:09.465974",
     "exception": false,
     "start_time": "2022-08-02T09:26:09.135554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+-----+-----+----+-----------+-----+---------+--------------+-------------+\n",
      "|PassengerId|Pclass| Age|SibSp|Parch|Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+-----------+------+----+-----+-----+----+-----------+-----+---------+--------------+-------------+\n",
      "|       1158|     1|32.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       1264|     1|49.0|    0|    0| 0.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "+-----------+------+----+-----+-----+----+-----------+-----+---------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.filter(df_test[\"Fare\"]==0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6459196f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:09.580436Z",
     "iopub.status.busy": "2022-08-02T09:26:09.579892Z",
     "iopub.status.idle": "2022-08-02T09:26:09.754734Z",
     "shell.execute_reply": "2022-08-02T09:26:09.753438Z"
    },
    "papermill": {
     "duration": 0.231601,
     "end_time": "2022-08-02T09:26:09.757828",
     "exception": false,
     "start_time": "2022-08-02T09:26:09.526227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Pclass=1, avg(Fare)=94.28029719626169),\n",
       " Row(Pclass=3, avg(Fare)=12.459677880184334),\n",
       " Row(Pclass=2, avg(Fare)=22.20210430107527)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the average fares per class\n",
    "df_test.groupby('Pclass').avg('Fare').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2b95703c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:09.880786Z",
     "iopub.status.busy": "2022-08-02T09:26:09.880366Z",
     "iopub.status.idle": "2022-08-02T09:26:09.929438Z",
     "shell.execute_reply": "2022-08-02T09:26:09.928496Z"
    },
    "papermill": {
     "duration": 0.105019,
     "end_time": "2022-08-02T09:26:09.932077",
     "exception": false,
     "start_time": "2022-08-02T09:26:09.827058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = df_test.withColumn(\"Fare\",when((df_test[\"Pclass\"] == 1) & (df_test[\"Fare\"]==0), 94).otherwise(df_test[\"Fare\"]))\n",
    "df_test = df_test.withColumn(\"Fare\",when((df_test[\"Pclass\"] == 2) & (df_test[\"Fare\"]==0), 22).otherwise(df_test[\"Fare\"]))\n",
    "df_test = df_test.withColumn(\"Fare\",when((df_test[\"Pclass\"] == 3) & (df_test[\"Fare\"]==0), 13).otherwise(df_test[\"Fare\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "61d8c513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:10.041736Z",
     "iopub.status.busy": "2022-08-02T09:26:10.040909Z",
     "iopub.status.idle": "2022-08-02T09:26:10.269100Z",
     "shell.execute_reply": "2022-08-02T09:26:10.268104Z"
    },
    "papermill": {
     "duration": 0.286981,
     "end_time": "2022-08-02T09:26:10.272515",
     "exception": false,
     "start_time": "2022-08-02T09:26:09.985534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|PassengerId|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+-----------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|        892|     3|34.5|    0|    0| 7.8292|          0|    1|      0.0|           2.0|          0.0|\n",
      "|        893|     3|47.0|    1|    0|    7.0|          1|    0|      1.0|           0.0|          2.0|\n",
      "|        894|     2|62.0|    0|    0| 9.6875|          0|    1|      0.0|           2.0|          0.0|\n",
      "|        895|     3|27.0|    0|    0| 8.6625|          0|    1|      0.0|           0.0|          0.0|\n",
      "|        896|     3|22.0|    1|    1|12.2875|          2|    0|      1.0|           0.0|          2.0|\n",
      "|        897|     3|14.0|    0|    0|  9.225|          0|    1|      0.0|           0.0|          0.0|\n",
      "|        898|     3|30.0|    0|    0| 7.6292|          0|    1|      1.0|           2.0|          1.0|\n",
      "|        899|     2|26.0|    1|    1|   29.0|          2|    0|      0.0|           0.0|          0.0|\n",
      "|        900|     3|18.0|    0|    0| 7.2292|          0|    1|      1.0|           1.0|          2.0|\n",
      "|        901|     3|21.0|    2|    0|  24.15|          2|    0|      0.0|           0.0|          0.0|\n",
      "|        902|     3|32.0|    0|    0| 7.8958|          0|    1|      0.0|           0.0|          0.0|\n",
      "|        903|     1|46.0|    0|    0|   26.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|        904|     1|23.0|    1|    0|82.2667|          1|    0|      1.0|           0.0|          2.0|\n",
      "|        905|     2|63.0|    1|    0|   26.0|          1|    0|      0.0|           0.0|          0.0|\n",
      "|        906|     1|47.0|    1|    0| 61.175|          1|    0|      1.0|           0.0|          2.0|\n",
      "|        907|     2|24.0|    1|    0|27.7208|          1|    0|      1.0|           1.0|          2.0|\n",
      "|        908|     2|35.0|    0|    0|  12.35|          0|    1|      0.0|           2.0|          0.0|\n",
      "|        909|     3|21.0|    0|    0|  7.225|          0|    1|      0.0|           1.0|          0.0|\n",
      "|        910|     3|27.0|    1|    0|  7.925|          1|    0|      1.0|           0.0|          1.0|\n",
      "|        911|     3|45.0|    0|    0|  7.225|          0|    1|      1.0|           1.0|          2.0|\n",
      "+-----------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "792e3118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:10.382839Z",
     "iopub.status.busy": "2022-08-02T09:26:10.382421Z",
     "iopub.status.idle": "2022-08-02T09:26:10.573025Z",
     "shell.execute_reply": "2022-08-02T09:26:10.571779Z"
    },
    "papermill": {
     "duration": 0.24825,
     "end_time": "2022-08-02T09:26:10.576848",
     "exception": false,
     "start_time": "2022-08-02T09:26:10.328598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|Initial_index|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "|       0|     3|22.0|    1|    0|   7.25|          1|    0|      0.0|           0.0|          0.0|\n",
      "|       1|     1|38.0|    1|    0|71.2833|          1|    0|      1.0|           1.0|          2.0|\n",
      "|       1|     3|26.0|    0|    0|  7.925|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       1|     1|35.0|    1|    0|   53.1|          1|    0|      1.0|           0.0|          2.0|\n",
      "|       0|     3|35.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|33.0|    0|    0| 8.4583|          0|    1|      0.0|           2.0|          0.0|\n",
      "|       0|     1|54.0|    0|    0|51.8625|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3| 2.0|    3|    1| 21.075|          4|    0|      0.0|           0.0|          3.0|\n",
      "|       1|     3|27.0|    0|    2|11.1333|          2|    0|      1.0|           0.0|          2.0|\n",
      "|       1|     2|14.0|    1|    0|30.0708|          1|    0|      1.0|           1.0|          2.0|\n",
      "|       1|     3| 4.0|    1|    1|   16.7|          2|    0|      1.0|           0.0|          1.0|\n",
      "|       1|     1|58.0|    0|    0|  26.55|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       0|     3|20.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|39.0|    1|    5| 31.275|          6|    0|      0.0|           0.0|          0.0|\n",
      "|       0|     3|14.0|    0|    0| 7.8542|          0|    1|      1.0|           0.0|          1.0|\n",
      "|       1|     2|55.0|    0|    0|   16.0|          0|    1|      1.0|           0.0|          2.0|\n",
      "|       0|     3| 2.0|    4|    1| 29.125|          5|    0|      0.0|           2.0|          3.0|\n",
      "|       1|     2|33.0|    0|    0|   13.0|          0|    1|      0.0|           0.0|          0.0|\n",
      "|       0|     3|31.0|    1|    0|   18.0|          1|    0|      1.0|           0.0|          2.0|\n",
      "|       1|     3|37.0|    0|    0|  7.225|          0|    1|      1.0|           1.0|          2.0|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d69528",
   "metadata": {
    "papermill": {
     "duration": 0.052284,
     "end_time": "2022-08-02T09:26:10.694026",
     "exception": false,
     "start_time": "2022-08-02T09:26:10.641742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**PREDICTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c581c21c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:10.800826Z",
     "iopub.status.busy": "2022-08-02T09:26:10.800404Z",
     "iopub.status.idle": "2022-08-02T09:26:10.810342Z",
     "shell.execute_reply": "2022-08-02T09:26:10.809381Z"
    },
    "papermill": {
     "duration": 0.066344,
     "end_time": "2022-08-02T09:26:10.812584",
     "exception": false,
     "start_time": "2022-08-02T09:26:10.746240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vec_asmbl = VectorAssembler(inputCols=df_train.columns[1:], \n",
    "                           outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7b465577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:10.920435Z",
     "iopub.status.busy": "2022-08-02T09:26:10.919709Z",
     "iopub.status.idle": "2022-08-02T09:26:10.924097Z",
     "shell.execute_reply": "2022-08-02T09:26:10.923220Z"
    },
    "papermill": {
     "duration": 0.060088,
     "end_time": "2022-08-02T09:26:10.926114",
     "exception": false,
     "start_time": "2022-08-02T09:26:10.866026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f456ebd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:11.034494Z",
     "iopub.status.busy": "2022-08-02T09:26:11.033731Z",
     "iopub.status.idle": "2022-08-02T09:26:11.067771Z",
     "shell.execute_reply": "2022-08-02T09:26:11.066744Z"
    },
    "papermill": {
     "duration": 0.091061,
     "end_time": "2022-08-02T09:26:11.070146",
     "exception": false,
     "start_time": "2022-08-02T09:26:10.979085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol='Survived', \n",
    "                           numTrees=100, maxDepth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e474b998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:26:11.177565Z",
     "iopub.status.busy": "2022-08-02T09:26:11.176351Z",
     "iopub.status.idle": "2022-08-02T09:29:15.373163Z",
     "shell.execute_reply": "2022-08-02T09:29:15.371899Z"
    },
    "papermill": {
     "duration": 184.253261,
     "end_time": "2022-08-02T09:29:15.375892",
     "exception": false,
     "start_time": "2022-08-02T09:26:11.122631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:26:15 WARN DAGScheduler: Broadcasting large task binary with size 1124.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:26:17 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/08/02 09:26:19 WARN DAGScheduler: Broadcasting large task binary with size 1086.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:26:21 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/08/02 09:26:23 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:26:25 WARN DAGScheduler: Broadcasting large task binary with size 1124.2 KiB\n",
      "22/08/02 09:26:26 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:26:29 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "22/08/02 09:26:31 WARN DAGScheduler: Broadcasting large task binary with size 1086.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:26:32 WARN DAGScheduler: Broadcasting large task binary with size 1873.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:26:34 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "22/08/02 09:26:36 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:26:38 WARN DAGScheduler: Broadcasting large task binary with size 1124.2 KiB\n",
      "22/08/02 09:26:39 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:26:41 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:26:43 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "22/08/02 09:26:45 WARN DAGScheduler: Broadcasting large task binary with size 1086.7 KiB\n",
      "22/08/02 09:26:46 WARN DAGScheduler: Broadcasting large task binary with size 1873.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:26:47 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:26:49 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "22/08/02 09:26:51 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:26:54 WARN DAGScheduler: Broadcasting large task binary with size 1130.7 KiB\n",
      "22/08/02 09:26:55 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/08/02 09:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1087.6 KiB\n",
      "22/08/02 09:26:59 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/08/02 09:27:01 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:27:02 WARN DAGScheduler: Broadcasting large task binary with size 1130.7 KiB\n",
      "22/08/02 09:27:03 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:06 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "22/08/02 09:27:07 WARN DAGScheduler: Broadcasting large task binary with size 1087.6 KiB\n",
      "22/08/02 09:27:08 WARN DAGScheduler: Broadcasting large task binary with size 1860.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:10 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "22/08/02 09:27:12 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:27:13 WARN DAGScheduler: Broadcasting large task binary with size 1130.7 KiB\n",
      "22/08/02 09:27:14 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:16 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:19 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "22/08/02 09:27:21 WARN DAGScheduler: Broadcasting large task binary with size 1087.6 KiB\n",
      "22/08/02 09:27:21 WARN DAGScheduler: Broadcasting large task binary with size 1860.7 KiB\n",
      "22/08/02 09:27:23 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:24 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "22/08/02 09:27:26 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:27:28 WARN DAGScheduler: Broadcasting large task binary with size 1131.1 KiB\n",
      "22/08/02 09:27:30 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "22/08/02 09:27:32 WARN DAGScheduler: Broadcasting large task binary with size 1111.1 KiB\n",
      "22/08/02 09:27:33 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "22/08/02 09:27:35 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:27:37 WARN DAGScheduler: Broadcasting large task binary with size 1131.1 KiB\n",
      "22/08/02 09:27:38 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:40 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "22/08/02 09:27:41 WARN DAGScheduler: Broadcasting large task binary with size 1111.1 KiB\n",
      "22/08/02 09:27:42 WARN DAGScheduler: Broadcasting large task binary with size 1974.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:44 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "22/08/02 09:27:46 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:27:48 WARN DAGScheduler: Broadcasting large task binary with size 1131.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:49 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:50 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:53 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "22/08/02 09:27:55 WARN DAGScheduler: Broadcasting large task binary with size 1111.1 KiB\n",
      "22/08/02 09:27:56 WARN DAGScheduler: Broadcasting large task binary with size 1974.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:57 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:27:59 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "22/08/02 09:28:01 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:28:03 WARN DAGScheduler: Broadcasting large task binary with size 1131.8 KiB\n",
      "22/08/02 09:28:04 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "22/08/02 09:28:06 WARN DAGScheduler: Broadcasting large task binary with size 1103.7 KiB\n",
      "22/08/02 09:28:07 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/08/02 09:28:09 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:28:11 WARN DAGScheduler: Broadcasting large task binary with size 1131.8 KiB\n",
      "22/08/02 09:28:12 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:28:14 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "22/08/02 09:28:16 WARN DAGScheduler: Broadcasting large task binary with size 1103.7 KiB\n",
      "22/08/02 09:28:17 WARN DAGScheduler: Broadcasting large task binary with size 1912.4 KiB\n",
      "22/08/02 09:28:18 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "22/08/02 09:28:20 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:28:22 WARN DAGScheduler: Broadcasting large task binary with size 1131.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:28:23 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:28:24 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:28:27 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "22/08/02 09:28:29 WARN DAGScheduler: Broadcasting large task binary with size 1103.7 KiB\n",
      "22/08/02 09:28:30 WARN DAGScheduler: Broadcasting large task binary with size 1912.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:28:31 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:28:33 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "22/08/02 09:28:35 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:28:37 WARN DAGScheduler: Broadcasting large task binary with size 1127.3 KiB\n",
      "22/08/02 09:28:38 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/08/02 09:28:40 WARN DAGScheduler: Broadcasting large task binary with size 1093.8 KiB\n",
      "22/08/02 09:28:41 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "22/08/02 09:28:43 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:28:45 WARN DAGScheduler: Broadcasting large task binary with size 1127.3 KiB\n",
      "22/08/02 09:28:46 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:28:48 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "22/08/02 09:28:50 WARN DAGScheduler: Broadcasting large task binary with size 1093.8 KiB\n",
      "22/08/02 09:28:50 WARN DAGScheduler: Broadcasting large task binary with size 1892.7 KiB\n",
      "22/08/02 09:28:52 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "22/08/02 09:28:54 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:28:55 WARN DAGScheduler: Broadcasting large task binary with size 1127.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:28:56 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:28:58 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:29:00 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "22/08/02 09:29:02 WARN DAGScheduler: Broadcasting large task binary with size 1093.8 KiB\n",
      "22/08/02 09:29:03 WARN DAGScheduler: Broadcasting large task binary with size 1892.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:29:04 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:29:06 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "22/08/02 09:29:08 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "22/08/02 09:29:10 WARN DAGScheduler: Broadcasting large task binary with size 1134.7 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:29:11 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:29:14 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.835016835016835"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline_rf = Pipeline(stages=[vec_asmbl, rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder().\\\n",
    "            addGrid(rf.maxDepth, [3, 4, 5]).\\\n",
    "            addGrid(rf.minInfoGain, [0., 0.01, 0.1]).\\\n",
    "            addGrid(rf.numTrees, [1000]).\\\n",
    "            build()\n",
    "\n",
    "selected_model = CrossValidator(estimator=pipeline_rf, \n",
    "                                estimatorParamMaps=paramGrid, \n",
    "                                evaluator=evaluator, \n",
    "                                numFolds=5)\n",
    "\n",
    "model_final = selected_model.fit(df_train)\n",
    "pred_train = model_final.transform(df_train)\n",
    "evaluator.evaluate(pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "af7be100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:29:15.507203Z",
     "iopub.status.busy": "2022-08-02T09:29:15.506798Z",
     "iopub.status.idle": "2022-08-02T09:29:15.583347Z",
     "shell.execute_reply": "2022-08-02T09:29:15.582033Z"
    },
    "papermill": {
     "duration": 0.145863,
     "end_time": "2022-08-02T09:29:15.586341",
     "exception": false,
     "start_time": "2022-08-02T09:29:15.440478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_test = model_final.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "48284d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:29:15.716439Z",
     "iopub.status.busy": "2022-08-02T09:29:15.715746Z",
     "iopub.status.idle": "2022-08-02T09:29:16.221747Z",
     "shell.execute_reply": "2022-08-02T09:29:16.220645Z"
    },
    "papermill": {
     "duration": 0.574872,
     "end_time": "2022-08-02T09:29:16.225332",
     "exception": false,
     "start_time": "2022-08-02T09:29:15.650460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:29:16 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "+-----------+--------+\n",
      "|PassengerId|Survived|\n",
      "+-----------+--------+\n",
      "|        892|       0|\n",
      "|        893|       1|\n",
      "|        894|       0|\n",
      "|        895|       0|\n",
      "|        896|       1|\n",
      "+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = pred_test.select('PassengerId', 'prediction')\n",
    "predictions = predictions.\\\n",
    "                withColumn('Survived', predictions['prediction'].\\\n",
    "                cast('integer')).drop('prediction')\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e1290c9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:29:16.364167Z",
     "iopub.status.busy": "2022-08-02T09:29:16.363766Z",
     "iopub.status.idle": "2022-08-02T09:29:16.845144Z",
     "shell.execute_reply": "2022-08-02T09:29:16.844187Z"
    },
    "papermill": {
     "duration": 0.550289,
     "end_time": "2022-08-02T09:29:16.847854",
     "exception": false,
     "start_time": "2022-08-02T09:29:16.297565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:29:16 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "+-----------+--------+\n",
      "|PassengerId|Survived|\n",
      "+-----------+--------+\n",
      "|        892|       0|\n",
      "|        893|       1|\n",
      "|        894|       0|\n",
      "|        895|       0|\n",
      "|        896|       1|\n",
      "|        897|       0|\n",
      "|        898|       1|\n",
      "|        899|       0|\n",
      "|        900|       1|\n",
      "|        901|       0|\n",
      "|        902|       0|\n",
      "|        903|       0|\n",
      "|        904|       1|\n",
      "|        905|       0|\n",
      "|        906|       1|\n",
      "|        907|       1|\n",
      "|        908|       0|\n",
      "|        909|       0|\n",
      "|        910|       1|\n",
      "|        911|       1|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c996e088",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:29:16.983924Z",
     "iopub.status.busy": "2022-08-02T09:29:16.983493Z",
     "iopub.status.idle": "2022-08-02T09:29:16.990565Z",
     "shell.execute_reply": "2022-08-02T09:29:16.989754Z"
    },
    "papermill": {
     "duration": 0.076542,
     "end_time": "2022-08-02T09:29:16.992821",
     "exception": false,
     "start_time": "2022-08-02T09:29:16.916279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('PassengerId', IntegerType(), True), StructField('Survived', IntegerType(), True)])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "26030149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T09:29:17.125420Z",
     "iopub.status.busy": "2022-08-02T09:29:17.124279Z",
     "iopub.status.idle": "2022-08-02T09:29:18.605711Z",
     "shell.execute_reply": "2022-08-02T09:29:18.602977Z"
    },
    "papermill": {
     "duration": 1.550325,
     "end_time": "2022-08-02T09:29:18.608246",
     "exception": true,
     "start_time": "2022-08-02T09:29:17.057921",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 09:29:17 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "22/08/02 09:29:18 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4083/0x00000008416c9040: (struct<Pclass_double_VectorAssembler_f80f71f1d249:double,Age:double,SibSp_double_VectorAssembler_f80f71f1d249:double,Parch_double_VectorAssembler_f80f71f1d249:double,Fare:double,Family_Size_double_VectorAssembler_f80f71f1d249:double,Alone_double_VectorAssembler_f80f71f1d249:double,Sex_index:double,Embarked_index:double,Initial_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 17 more\n",
      "22/08/02 09:29:18 ERROR FileFormatWriter: Job job_202208020929173181470275044804812_0931 aborted.\n",
      "22/08/02 09:29:18 ERROR Executor: Exception in task 0.0 in stage 931.0 (TID 852)\n",
      "org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4083/0x00000008416c9040: (struct<Pclass_double_VectorAssembler_f80f71f1d249:double,Age:double,SibSp_double_VectorAssembler_f80f71f1d249:double,Parch_double_VectorAssembler_f80f71f1d249:double,Fare:double,Family_Size_double_VectorAssembler_f80f71f1d249:double,Alone_double_VectorAssembler_f80f71f1d249:double,Sex_index:double,Embarked_index:double,Initial_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\t... 9 more\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 17 more\n",
      "22/08/02 09:29:18 WARN TaskSetManager: Lost task 0.0 in stage 931.0 (TID 852) (919f9cf1e03f executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4083/0x00000008416c9040: (struct<Pclass_double_VectorAssembler_f80f71f1d249:double,Age:double,SibSp_double_VectorAssembler_f80f71f1d249:double,Parch_double_VectorAssembler_f80f71f1d249:double,Fare:double,Family_Size_double_VectorAssembler_f80f71f1d249:double,Alone_double_VectorAssembler_f80f71f1d249:double,Sex_index:double,Embarked_index:double,Initial_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\t... 9 more\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 17 more\n",
      "\n",
      "22/08/02 09:29:18 ERROR TaskSetManager: Task 0 in stage 931.0 failed 1 times; aborting job\n",
      "22/08/02 09:29:18 ERROR FileFormatWriter: Aborting job 876f3ccc-9ab1-4018-97c3-a7419debfcd2.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 931.0 failed 1 times, most recent failure: Lost task 0.0 in stage 931.0 (TID 852) (919f9cf1e03f executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4083/0x00000008416c9040: (struct<Pclass_double_VectorAssembler_f80f71f1d249:double,Age:double,SibSp_double_VectorAssembler_f80f71f1d249:double,Parch_double_VectorAssembler_f80f71f1d249:double,Fare:double,Family_Size_double_VectorAssembler_f80f71f1d249:double,Alone_double_VectorAssembler_f80f71f1d249:double,Sex_index:double,Embarked_index:double,Initial_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\t... 9 more\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 17 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4083/0x00000008416c9040: (struct<Pclass_double_VectorAssembler_f80f71f1d249:double,Age:double,SibSp_double_VectorAssembler_f80f71f1d249:double,Parch_double_VectorAssembler_f80f71f1d249:double,Fare:double,Family_Size_double_VectorAssembler_f80f71f1d249:double,Alone_double_VectorAssembler_f80f71f1d249:double,Sex_index:double,Embarked_index:double,Initial_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n",
      "\t... 9 more\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 17 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o16924.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 931.0 failed 1 times, most recent failure: Lost task 0.0 in stage 931.0 (TID 852) (919f9cf1e03f executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4083/0x00000008416c9040: (struct<Pclass_double_VectorAssembler_f80f71f1d249:double,Age:double,SibSp_double_VectorAssembler_f80f71f1d249:double,Parch_double_VectorAssembler_f80f71f1d249:double,Fare:double,Family_Size_double_VectorAssembler_f80f71f1d249:double,Alone_double_VectorAssembler_f80f71f1d249:double,Sex_index:double,Embarked_index:double,Initial_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4083/0x00000008416c9040: (struct<Pclass_double_VectorAssembler_f80f71f1d249:double,Age:double,SibSp_double_VectorAssembler_f80f71f1d249:double,Parch_double_VectorAssembler_f80f71f1d249:double,Fare:double,Family_Size_double_VectorAssembler_f80f71f1d249:double,Alone_double_VectorAssembler_f80f71f1d249:double,Sex_index:double,Embarked_index:double,Initial_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 17 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/76785755.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Writing csv file in Spark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission_file.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         )\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     def orc(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o16924.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 931.0 failed 1 times, most recent failure: Lost task 0.0 in stage 931.0 (TID 852) (919f9cf1e03f executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4083/0x00000008416c9040: (struct<Pclass_double_VectorAssembler_f80f71f1d249:double,Age:double,SibSp_double_VectorAssembler_f80f71f1d249:double,Parch_double_VectorAssembler_f80f71f1d249:double,Fare:double,Family_Size_double_VectorAssembler_f80f71f1d249:double,Alone_double_VectorAssembler_f80f71f1d249:double,Sex_index:double,Embarked_index:double,Initial_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:642)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:348)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (VectorAssembler$$Lambda$4083/0x00000008416c9040: (struct<Pclass_double_VectorAssembler_f80f71f1d249:double,Age:double,SibSp_double_VectorAssembler_f80f71f1d249:double,Parch_double_VectorAssembler_f80f71f1d249:double,Fare:double,Family_Size_double_VectorAssembler_f80f71f1d249:double,Alone_double_VectorAssembler_f80f71f1d249:double,Sex_index:double,Embarked_index:double,Initial_index:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:331)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:338)\n\t... 9 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "# Writing csv file in Spark \n",
    "predictions.coalesce(1).write.csv('submission_file.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343546a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading the saved file from spark \n",
    "spark.read.csv('submission_file.csv', header=True).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5602a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 362.256841,
   "end_time": "2022-08-02T09:29:21.298328",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-02T09:23:19.041487",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
